{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Flight Interconnected Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Project Overview](#project-overview)\n",
    "2. [Data Source Description](#data-source-description)\n",
    "3. [Project Steps](#project-steps)\n",
    "4. [Conclusion](#conclusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Project Overview\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Source Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Projet Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Initialize PySpark Session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/26 18:11:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Page Rank Flights\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
    "    .config(\"spark.shuffle.service.enabled\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"100\") \\\n",
    "    .config(\"spark.ui.retainedStages\", 100) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/26 18:11:48 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------------+------+----+------------+--------+---------+--------+----------+---------+-------+------------+--------+---------+---------+-----------------+--------+----------------+-------------------+--------+--------+-------------+-------------+---------+--------------+-------------------+-----------+\n",
      "|   FL_DATE|OP_CARRIER|OP_CARRIER_FL_NUM|ORIGIN|DEST|CRS_DEP_TIME|DEP_TIME|DEP_DELAY|TAXI_OUT|WHEELS_OFF|WHEELS_ON|TAXI_IN|CRS_ARR_TIME|ARR_TIME|ARR_DELAY|CANCELLED|CANCELLATION_CODE|DIVERTED|CRS_ELAPSED_TIME|ACTUAL_ELAPSED_TIME|AIR_TIME|DISTANCE|CARRIER_DELAY|WEATHER_DELAY|NAS_DELAY|SECURITY_DELAY|LATE_AIRCRAFT_DELAY|Unnamed: 27|\n",
      "+----------+----------+-----------------+------+----+------------+--------+---------+--------+----------+---------+-------+------------+--------+---------+---------+-----------------+--------+----------------+-------------------+--------+--------+-------------+-------------+---------+--------------+-------------------+-----------+\n",
      "|2018-01-01|        UA|             2429|   EWR| DEN|        1517|  1512.0|     -5.0|    15.0|    1527.0|   1712.0|   10.0|        1745|  1722.0|    -23.0|      0.0|             null|     0.0|           268.0|              250.0|   225.0|  1605.0|         null|         null|     null|          null|               null|       null|\n",
      "|2018-01-01|        UA|             2427|   LAS| SFO|        1115|  1107.0|     -8.0|    11.0|    1118.0|   1223.0|    7.0|        1254|  1230.0|    -24.0|      0.0|             null|     0.0|            99.0|               83.0|    65.0|   414.0|         null|         null|     null|          null|               null|       null|\n",
      "|2018-01-01|        UA|             2426|   SNA| DEN|        1335|  1330.0|     -5.0|    15.0|    1345.0|   1631.0|    5.0|        1649|  1636.0|    -13.0|      0.0|             null|     0.0|           134.0|              126.0|   106.0|   846.0|         null|         null|     null|          null|               null|       null|\n",
      "|2018-01-01|        UA|             2425|   RSW| ORD|        1546|  1552.0|      6.0|    19.0|    1611.0|   1748.0|    6.0|        1756|  1754.0|     -2.0|      0.0|             null|     0.0|           190.0|              182.0|   157.0|  1120.0|         null|         null|     null|          null|               null|       null|\n",
      "|2018-01-01|        UA|             2424|   ORD| ALB|         630|   650.0|     20.0|    13.0|     703.0|    926.0|   10.0|         922|   936.0|     14.0|      0.0|             null|     0.0|           112.0|              106.0|    83.0|   723.0|         null|         null|     null|          null|               null|       null|\n",
      "+----------+----------+-----------------+------+----+------------+--------+---------+--------+----------+---------+-------+------------+--------+---------+---------+-----------------+--------+----------------+-------------------+--------+--------+-------------+-------------+---------+--------------+-------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "flights_df = spark.read.csv(\"../data/2018.csv\", header=True, inferSchema=True)\n",
    "flights_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Exploring Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- FL_DATE: string (nullable = true)\n",
      " |-- OP_CARRIER: string (nullable = true)\n",
      " |-- OP_CARRIER_FL_NUM: integer (nullable = true)\n",
      " |-- ORIGIN: string (nullable = true)\n",
      " |-- DEST: string (nullable = true)\n",
      " |-- CRS_DEP_TIME: integer (nullable = true)\n",
      " |-- DEP_TIME: double (nullable = true)\n",
      " |-- DEP_DELAY: double (nullable = true)\n",
      " |-- TAXI_OUT: double (nullable = true)\n",
      " |-- WHEELS_OFF: double (nullable = true)\n",
      " |-- WHEELS_ON: double (nullable = true)\n",
      " |-- TAXI_IN: double (nullable = true)\n",
      " |-- CRS_ARR_TIME: integer (nullable = true)\n",
      " |-- ARR_TIME: double (nullable = true)\n",
      " |-- ARR_DELAY: double (nullable = true)\n",
      " |-- CANCELLED: double (nullable = true)\n",
      " |-- CANCELLATION_CODE: string (nullable = true)\n",
      " |-- DIVERTED: double (nullable = true)\n",
      " |-- CRS_ELAPSED_TIME: double (nullable = true)\n",
      " |-- ACTUAL_ELAPSED_TIME: double (nullable = true)\n",
      " |-- AIR_TIME: double (nullable = true)\n",
      " |-- DISTANCE: double (nullable = true)\n",
      " |-- CARRIER_DELAY: double (nullable = true)\n",
      " |-- WEATHER_DELAY: double (nullable = true)\n",
      " |-- NAS_DELAY: double (nullable = true)\n",
      " |-- SECURITY_DELAY: double (nullable = true)\n",
      " |-- LATE_AIRCRAFT_DELAY: double (nullable = true)\n",
      " |-- Unnamed: 27: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Given the schema, we can define the vertices and edges as follows:\n",
    "\n",
    "* Vertices: Unique airports from the ORIGIN and DEST columns.\n",
    "* Edges: Each flight from an origin airport to a destination airport. Weights are the count of number of flights from origin to dest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Data Cleaning and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_df = flights_df.na.drop(subset=[\"ORIGIN\", \"DEST\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.1 Defining vertics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:=======================================>              (147 + 15) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|BNA|\n",
      "|CLT|\n",
      "|TVC|\n",
      "|CLL|\n",
      "|CGI|\n",
      "+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Creating vertices DataFrame\n",
    "airports_df = flights_df.select(\"ORIGIN\").distinct().union(\n",
    "    flights_df.select(\"DEST\").distinct()\n",
    ").distinct().withColumnRenamed(\"ORIGIN\", \"id\")\n",
    "\n",
    "# Displaying the vertices DataFrame\n",
    "airports_df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.2 Defining edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_df = flights_df.groupBy(\"ORIGIN\", \"DEST\").agg(\n",
    "    count(\"*\").alias(\"flight_count\")\n",
    ").withColumnRenamed(\"ORIGIN\", \"src\").withColumnRenamed(\"DEST\", \"dst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:=================================>                        (7 + 5) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------------+\n",
      "|src|dst|flight_count|\n",
      "+---+---+------------+\n",
      "|ANC|DEN|         459|\n",
      "|BNA|IAH|        1864|\n",
      "|TPA|SFO|         381|\n",
      "|ORD|PDX|        2762|\n",
      "|ANC|OTZ|         671|\n",
      "+---+---+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "edges_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Implementing Page Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_df = airports_df.withColumn(\"rank\", lit(1.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "damping_factor = 0.85\n",
    "max_iter = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in range(max_iter):\n",
    "    # Join edges with ranks\n",
    "    ranks_with_edges = edges_df.join(airports_df, edges_df.src == airports_df.id)\\\n",
    "                               .select(\"dst\", \"rank\")\n",
    "\n",
    "    # Calculate contributions\n",
    "    contributions = ranks_with_edges.groupBy(\"dst\")\\\n",
    "                                    .agg(sum(\"rank\").alias(\"contributions\"))\n",
    "\n",
    "    # Update ranks\n",
    "    airports_df = airports_df.join(contributions, airports_df.id == contributions.dst, \"left_outer\")\\\n",
    "                             .select(airports_df.id, \n",
    "                                     (col(\"contributions\") * damping_factor + (1 - damping_factor)).alias(\"rank\"))\n",
    "    \n",
    "    # Handling vertices without incoming edges\n",
    "    airports_df = airports_df.na.fill(1.0 - damping_factor, [\"rank\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 36:=============================================>        (84 + 12) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|                rank|\n",
      "+---+--------------------+\n",
      "|ATL|8.291052955909923...|\n",
      "|ORD|8.126526736172300...|\n",
      "|DEN|8.029572613266470...|\n",
      "|DFW|7.897698069076595...|\n",
      "|CLT|7.442846123136857...|\n",
      "+---+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "airports_df.orderBy(\"rank\", ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ATL: Hartsfield-Jackson Atlanta International Airport\n",
    "- ORD: Chicago O'Hare International Airport\n",
    "- DEN: Denver International Airport\n",
    "- DFW: Dallas/Fort Worth International Airport\n",
    "- CLT: Charlotte Douglas International Airport\n",
    "- MSP: Minneapolis-Saint Paul International Airport\n",
    "- LAS: McCarran International Airport (Las Vegas)\n",
    "- DTW: Detroit Metropolitan Wayne County Airport\n",
    "- IAH: George Bush Intercontinental Airport (Houston)\n",
    "- MCO: Orlando International Airport\n",
    "- PHX: Phoenix Sky Harbor International Airport\n",
    "- EWR: Newark Liberty International Airport\n",
    "- LAX: Los Angeles International Airport\n",
    "- PHL: Philadelphia International Airport\n",
    "- DCA: Ronald Reagan Washington National Airport\n",
    "- SLC: Salt Lake City International Airport\n",
    "- FLL: Fort Lauderdale-Hollywood International Airport\n",
    "- BWI: Baltimore/Washington International Thurgood Marshall Airport\n",
    "- SEA: Seattle-Tacoma International Airport\n",
    "- SFO: San Francisco International Airport\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using gephi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Assuming 'airports_df' contains the final PageRank values\n",
    "airports_df.select(\"id\", \"rank\").write.csv(\"../airoports_temp.csv\", header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "edges_df = edges_df.withColumnRenamed(\"src\", \"Source\").withColumnRenamed(\"dst\", \"Target\")\n",
    "edges_df.coalesce(1).write.csv(\"../edges_temp.csv\", header=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preparation for Gephi Visualization\n",
    "\n",
    "1. Set permissions for edges_temp.csv and airports_temp.csv:\n",
    "\n",
    "   ```bash\n",
    "   sudo chmod -R 777 edges_temp.csv\n",
    "   sudo chmod -R 777 airoports_temp.csv\n",
    "2. Consolidates all CSV files within the edges_temp.csv directory into a single file named edges.csv. This step is essential for organizing the data for Gephi visualization, especially when we save distributed data into separate CSV files.\n",
    "   ```bash\n",
    "   cat edges_temp.csv/*.csv > edges.csv\n",
    "   cat airoports_temp.csv/*.csv > airports.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Plotly to show top 50 graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_airports_df = airports_df.orderBy(\"rank\", ascending=False).limit(50)\n",
    "# Get a list of top 10 airport IDs\n",
    "top_airport_ids = top_airports_df.select(\"id\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Filter edges to include only those that connect the top 10 airports\n",
    "top_edges_df = edges_df.filter(edges_df.src.isin(top_airport_ids) & edges_df.dst.isin(top_airport_ids))\n",
    "import networkx as nx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Convert Spark DataFrame to Pandas DataFrame\n",
    "top_airports_pd = top_airports_df.toPandas()\n",
    "top_edges_pd = top_edges_df.toPandas()\n",
    "\n",
    "\n",
    "# Assuming you have top_airports_df and top_edges_df as Pandas DataFrames\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add nodes with attributes (e.g., PageRank scores)\n",
    "for idx, row in top_airports_pd.iterrows():\n",
    "    G.add_node(row['id'], rank=row['rank'])\n",
    "\n",
    "# Add edges\n",
    "for idx, row in top_edges_pd.iterrows():\n",
    "    G.add_edge(row['src'], row['dst'])\n",
    "    \n",
    "nx.write_gexf(G, \"../graph.gexf\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "import networkx as nx\n",
    "\n",
    "# Assuming G is your graph and pos is the position dictionary for nodes\n",
    "pos = nx.spring_layout(G)\n",
    "\n",
    "# Edges\n",
    "edge_x = []\n",
    "edge_y = []\n",
    "for edge in G.edges():\n",
    "    x0, y0 = pos[edge[0]]\n",
    "    x1, y1 = pos[edge[1]]\n",
    "    edge_x.extend([x0, x1, None])\n",
    "    edge_y.extend([y0, y1, None])\n",
    "\n",
    "edge_trace = go.Scatter(\n",
    "    x=edge_x, y=edge_y,\n",
    "    line=dict(width=0.5, color='#888'),\n",
    "    hoverinfo='none',\n",
    "    mode='lines')\n",
    "\n",
    "# Nodes\n",
    "node_x = []\n",
    "node_y = []\n",
    "for node in G.nodes():\n",
    "    x, y = pos[node]\n",
    "    node_x.append(x)\n",
    "    node_y.append(y)\n",
    "\n",
    "# Calculate node sizes based on PageRank\n",
    "max_rank = top_airports_pd['rank'].max()\n",
    "node_sizes = [10 + 20 * (rank / max_rank) for rank in top_airports_pd['rank']]\n",
    "\n",
    "node_trace = go.Scatter(\n",
    "    x=node_x, y=node_y,\n",
    "    mode='markers',\n",
    "    hoverinfo='text',\n",
    "    marker=dict(\n",
    "        showscale=True,\n",
    "        colorscale='YlGnBu',\n",
    "        reversescale=True,\n",
    "        size=node_sizes,  # Use the calculated sizes\n",
    "        colorbar=dict(\n",
    "            thickness=15,\n",
    "            title='Node Connections',\n",
    "            xanchor='left',\n",
    "            titleside='right'\n",
    "        ),\n",
    "        line_width=2))\n",
    "\n",
    "# Draw the graph\n",
    "fig = go.Figure(data=[edge_trace, node_trace],\n",
    "                layout=go.Layout(\n",
    "                    title='<br>Network graph made with Python',\n",
    "                    titlefont_size=16,\n",
    "                    showlegend=False,\n",
    "                    hovermode='closest',\n",
    "                    margin=dict(b=20,l=5,r=5,t=40),\n",
    "                    xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "                    yaxis=dict(showgrid=False, zeroline=False, showticklabels=False))\n",
    "                )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.write_html(\"../test1.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
